{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7fe76e843a95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mscanpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0manndata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_latest/lib/python3.7/site-packages/scanpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# the actual API\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_settings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVerbosity\u001b[0m  \u001b[0;31m# start with settings as several tools are using it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtools\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocessing\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplotting\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_latest/lib/python3.7/site-packages/scanpy/tools/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_dpt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdpt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_leiden\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mleiden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_louvain\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlouvain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_sim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_score_genes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscore_genes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_genes_cell_cycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_latest/lib/python3.7/site-packages/scanpy/tools/_louvain.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mlouvain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVertexPartition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMutableVertexPartition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mMutableVertexPartition\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_latest/lib/python3.7/site-packages/louvain/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mimmediately\u001b[0m \u001b[0mavailable\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mlouvain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_partition\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \"\"\"\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mALL_COMMS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mALL_NEIGH_COMMS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRAND_COMM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_latest/lib/python3.7/site-packages/louvain/functions.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0migraph\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_ig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_c_louvain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_c_louvain\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mALL_COMMS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_c_louvain\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mALL_NEIGH_COMMS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy.api as sc\n",
    "import anndata\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import scipy\n",
    "from scipy import sparse\n",
    "import sys\n",
    "\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import tensorboard\n",
    "from keras.layers import Dense, Flatten, Reshape, BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from umap import UMAP\n",
    "import seaborn as sns \n",
    "\n",
    "sc.settings.verbosity = 3 \n",
    "sc.settings.set_figure_params(dpi=80)  # low dpi (dots per inch) yields small inline figures\n",
    "sc.logging.print_versions()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# open tensorboard and set log dir "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"./tmp/logs/\"\n",
    "summary_writer = tf.summary.create_file_writer(logdir=log_dir)\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./tmp/logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in already preprocessed input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "exp_mat = pd.read_csv(\"Gan_input_3_mg.csv\", sep='\\t', index_col = 0)\n",
    "exp_mat = exp_mat.T\n",
    "\n",
    "input_matrix = np.genfromtxt('Gan_input_3_mg.csv', skip_header=1)\n",
    "\n",
    "input_matrix = input_matrix.T\n",
    "input_matrix.shape\n",
    "\n",
    "\n",
    "input_matrix = np.delete(input_matrix, (0), axis=0)\n",
    "\n",
    "scaler.fit(input_matrix)\n",
    "input_matrix = scaler.transform(input_matrix)\n",
    "\n",
    "i  = np.random.randint(0, input_matrix[0].shape, 500)\n",
    "validation = input_matrix[i]\n",
    "input_matrix = np.delete(input_matrix, i, 0)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model input dimensions\n",
    "z_dim = 100\n",
    "cell = input_matrix[1].shape\n",
    "out_dim = cell[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_matrix[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator as a function. Returns the generator network model.\n",
    "\n",
    "def build_generator(cell, z_dim) :\n",
    "  \n",
    "  # Defines the model\n",
    "  model = Sequential()\n",
    "\n",
    "  # Adds a dense layer of 64 neurons with input_dim equal to z_dim\n",
    "  model.add(Dense(128, input_dim = z_dim))\n",
    "  \n",
    "\n",
    "  # Apply Leaky ReLU activaion function \n",
    "  model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "  model.add(BatchNormalization())\n",
    "\n",
    "  model.add(Dense(200))\n",
    "    \n",
    "  model.add(LeakyReLU(alpha=0.2))\n",
    "  \n",
    "  model.add(BatchNormalization())\n",
    "\n",
    "  # Adds another fully connected layer - output layer \n",
    "  model.add(Dense(out_dim, activation = 'tanh'))\n",
    "\n",
    "  model.add(Reshape(cell))\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Discrimator network\n",
    "\n",
    "def build_discriminator(cell) :\n",
    "\n",
    "  model = Sequential()\n",
    "\n",
    "  model.add(Dense(250, input_shape = cell))\n",
    "  \n",
    "  model.add(BatchNormalization())\n",
    "    \n",
    "  model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "  model.add(Dense(250))\n",
    "    \n",
    "  model.add(LeakyReLU(alpha=0.2))\n",
    "  \n",
    "  model.add(BatchNormalization())\n",
    "\n",
    "  model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the GAN\n",
    "\n",
    "def build_gan(generator, discrimator) :\n",
    "\n",
    "  model = Sequential()\n",
    "\n",
    "  model.add(generator)\n",
    "  model.add(discrimator)\n",
    "\n",
    "  return model\n",
    "\n",
    "discrimator = build_discriminator(cell)\n",
    "discrimator.compile(loss = 'binary_crossentropy',\n",
    "                    optimizer = Adam(),\n",
    "                    metrics = ['accuracy'])\n",
    "\n",
    "generator = build_generator(cell, z_dim)\n",
    "discrimator.trainable = False\n",
    "\n",
    "gan = build_gan(generator, discrimator)\n",
    "gan.compile(loss='binary_crossentropy', optimizer=Adam())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cells(number_cells) :\n",
    "    \n",
    "    # Draws from random normal.\n",
    "    z = np.random.normal(0, 1, (1, z_dim))\n",
    "    \n",
    "    # predcts using enerator network.\n",
    "    gen_cell = generator.predict(z)\n",
    "    \n",
    "    # Transform back from MinMax scaling between -1 and 1 (necesscary for tanh activation function) \n",
    "    gen_cell = scaler.inverse_transform(gen_cell)\n",
    "\n",
    "\n",
    "   # Repeat of above procedure inside a loop. \n",
    "    for cell in range(num_cells -1 ):\n",
    "    \n",
    "            z = np.random.normal(0, 1, (1, z_dim))\n",
    "    \n",
    "            gen_cells_tmp = generator.predict(z)\n",
    "    \n",
    "            gen_cells_tmp = scaler.inverse_transform(gen_cells_tmp)\n",
    "        \n",
    "          #  Appends subsequent generated cells to form an array of generated cells\n",
    "            gen_cell = np.append(gen_cell, gen_cells_tmp, axis=0)\n",
    "    \n",
    "    # Converts array to matrice object\n",
    "    gen_cell = np.asmatrix(gen_cell)\n",
    "    \n",
    "    # converts matrice to pandas dataframe\n",
    "    gen_cell = pd.DataFrame(gen_cell)\n",
    "    \n",
    "    # Loop to create cell names (theres deinitely a better way to do this)\n",
    "    cell_names = []\n",
    "    for i in range(num_cells ):\n",
    "        cell_names.append(\"cell-{}\".format(i +1))\n",
    "        \n",
    "        \n",
    "    # Creates final dataframe with gene names and cell IDs\n",
    "    gen_cell = pd.DataFrame(data=gen_cell.values, columns=exp_mat.columns, index = cell_names)\n",
    "    \n",
    "    return gen_cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accuracies = []\n",
    "iteration_checkpoints = []\n",
    "Generated_cells = []\n",
    "sample_checkpoints = []\n",
    "\n",
    "d_hist = []\n",
    "g_loss_hist = []\n",
    "\n",
    "def train(iterations, batch_size, sample_interval, generate_cells_every, num_cells) :\n",
    "\n",
    "  X_train =  input_matrix\n",
    "\n",
    "  real = np.ones((batch_size, 1))\n",
    "  fake = np.zeros((batch_size, 1))\n",
    "\n",
    "  for iteration in range(iterations) :\n",
    "\n",
    "    idx = np.random.randint(X_train.shape[0], size = batch_size)\n",
    "    real_cell = X_train[idx]\n",
    "\n",
    "    z = np.random.normal(0, 1, (batch_size, z_dim))\n",
    "    gen_cell = generator.predict(z)\n",
    "\n",
    "    d_loss_real = discrimator.train_on_batch(real_cell, real)\n",
    "    d_loss_fake = discrimator.train_on_batch(gen_cell, fake)\n",
    "    d_loss, accuracy = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "    d_hist.append(d_loss)\n",
    "    \n",
    "\n",
    "    z = np.random.normal(0,1, (batch_size, 100))\n",
    "    gen_imgs = generator.predict(z)\n",
    "    g_loss = gan.train_on_batch(z, real)\n",
    "    g_loss_hist.append(g_loss)\n",
    "    \n",
    "    with summary_writer.as_default():\n",
    "      tf.summary.scalar('d_loss', data=d_loss, step=iteration+1)\n",
    "      tf.summary.scalar('g_loss', data=g_loss, step=iteration+1)\n",
    "      tf.summary.scalar('accuracy', data=accuracy, step=iteration+1)\n",
    "\n",
    "\n",
    "    if (iteration + 1) % sample_interval == 0:\n",
    "\n",
    "      \n",
    "      accuracies.append(100.0 * accuracy)\n",
    "      iteration_checkpoints.append(iteration + 1)\n",
    "\n",
    "      print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" %\n",
    "            (iteration + 1, d_loss, 100.0 * accuracy, g_loss))\n",
    "        \n",
    "    if (iteration +1) % generate_cells_every == 0 or (iteration +1) == 1000:\n",
    "        gen_cell = generate_cells(500)\n",
    "        Generated_cells.append(gen_cell)\n",
    "        sample_checkpoints.append(iteration +1)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir figures/Vanilla_GAN_793_genes_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(c1_hist, g_hist, name):\n",
    "    plt.plot(c1_hist, label='Discriminator Loss', )\n",
    "    #plt.plot(g_hist, label='Generator loss')\n",
    "    plt.legend(fontsize = 'small')\n",
    "    plt.savefig('Discriminator_{}'.format(name))\n",
    "    plt.show()\n",
    "    plt.plot(g_hist, label='Generator loss', color = \"orange\")\n",
    "    plt.legend(fontsize = 'small')\n",
    "    plt.savefig('Generator_loss_{}'.format(name))\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running the model\n",
    "\n",
    "# Setting hyper parameters\n",
    "iterations = 20000\n",
    "batch_size = 32\n",
    "sample_interval = 1000\n",
    "generate_cells_every = 5000\n",
    "num_cells = 500\n",
    "\n",
    "train(iterations, batch_size, sample_interval, generate_cells_every, num_cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(d_hist, g_loss_hist, \"Vanilla_gan_loss_graph.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Generated_cells' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-40b95db76912>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mGenerated_cells\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_checkpoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcorr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Generated_cells' is not defined"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for dataset in Generated_cells:\n",
    "    checkpoint = sample_checkpoints[i]\n",
    "    gen = dataset.T\n",
    "    corr = gen.corr()\n",
    "    corr = corr.to_numpy()\n",
    "    corr= corr.flatten()\n",
    "    plt.figure()        \n",
    "    plt.hist(corr, density=True, range = (0,1))  # `density=False` would make counts\n",
    "    plt.title(label = \"{} Iterations\".format(checkpoint))\n",
    "    plt.ylabel('Density')\n",
    "    plt.xlabel('Correlation')\n",
    "    plt.savefig('figures/Vanilla_GAN_793_genes_results/{}'.format(checkpoint))\n",
    "    i = i +1;    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "for dataset in Generated_cells :\n",
    "    checkpoint = sample_checkpoints[i]\n",
    "    test = dataset\n",
    "    adata = sc.AnnData(test)\n",
    "    adata.X\n",
    "    sc.pp.neighbors(adata)\n",
    "    sc.tl.umap(adata)\n",
    "    sc.tl.louvain(adata, resolution=1, key_added='louvain_r1')\n",
    "    sc.pl.umap(adata, color='louvain_r1', save = \"vanilla_GAN_greyscale_Vanilla_GAN_UMAP_at_{}.png\".format(checkpoint), title = \"{} iterations\".format(checkpoint))\n",
    "    sc.tl.rank_genes_groups(adata, 'louvain_r1', method='logreg')\n",
    "    sc.pl.rank_genes_groups(adata, n_genes=20,  save =  \"Vanilla_GAN_greyscale_Vanilla_GAN_Gene_rank_at_{}.png\".format(checkpoint))\n",
    "    result = adata.uns['rank_genes_groups']\n",
    "    groups = result['names'].dtype.names\n",
    "    pd.DataFrame({group + '_' + key[:1]: result[key][group]\n",
    "    for group in groups for key in ['names', 'scores']}).head(10)\n",
    "    i = i +1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = pd.DataFrame(val)\n",
    "\n",
    "for dataset in Generated_cells:\n",
    "    gen = dataset\n",
    "    \n",
    "    corr = gen.corrwith(val, axis = 1) \n",
    "    \n",
    "    corr = np.array(list(corr))\n",
    "    \n",
    "    \n",
    "    plt.figure()        \n",
    "    plt.hist(corr, density=True, range = (0,1))  # `density=False` would make counts\n",
    "    plt.ylabel('Density')\n",
    "    plt.xlabel('Correlation');   \n",
    "\n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
